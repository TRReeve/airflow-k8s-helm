# Default values for airflow-kubernetes.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


airflow:
  image:
    # airflow.image.repository -- Airflow Image
    repository: reevedata/airflow-kubernetes
    # airflow.image.pullPolicy
    pullPolicy: Always
    # airflow.image.tag
    tag: 1.10.10
  exampleDags:
    # airflow.exampleDags.enable -- Create example dags
    enable: true
  # Normally will be preferable to set this up for any production process
  remoteLogging:
    # airflow.remoteLogging.enable -- enable remote logging to an s3/gcs..etc. Recommended as workers are ephemeral.
    enable: false
    # airflow.remoteLogging.connName -- Name you want to give the connection used for remote logging.
    connName: logging
    # airflow.remoteLogging.connType -- The type of connection you want to set up for remote logging. This chart has been tested
    # with s3 (AWS) and gs (GCP)
    connType:
    # airflow.remoteLogging.remoteLoggingPath -- the path logs will be written to in your remote storage bucket.
    path:
    # airflow.remoteLogging.createSecret -- Create a logging secret file
    # using the secret defined under remote_logging_secret This needs to contain the login:password or similar for
    #whatever makes up the username and password element of your connection URI
    # For example with S3 this would be ACCESS_KEY_ID:ACCESS_KEY_SECRET_ID.
    # This is then combined with the conn_type and the remote_logs path to create the uri that
    # airflow will use to build the connection. e.g {conn_type}://{remote_logging_secret}/remote_logs_path
    createSecret: true
    # airflow.remoteLogging.secretName -- The name of the secrets file providing a
    # REMOTE_LOGGING_SECRET that will be used to construct an airflow URI for the connection
    secretName: "logging-secrets"
    # airflow.remoteLogging.credential -- If create_logging_secret is true, the secret you want to use for remote logging.
    credential:
  webserver:
    # airflow.webserver.enable -- Whether to create webserver UI
    enable: true
    # airflow.webserver.repliacs -- Webserver replices
    replicas: 1
    ingress:
      # airflow.webserver.ingress.enable -- Whether to create an ingress for your webserver.
      enable: true
      # airflow.webserver.ingress.host -- Host for your ingress controller.
      host: airflow.helmdeploy.data.global.com
      # airflow.webserver.ingress.annotations -- Ingress annotations for your ingress controller.
      annotations:
    resources:
      requests:
        cpu: "0.1"
        memory: "500Mi"
  scheduler:
    requests:
      cpu: "0.5"
      memory: "500Mi"
  secrets:
    # airflow.secrets.createFile -- create a secrets file using the credentials provided in the db section and extra secrets sections.
    # or set to false and provide a secrets file providing DB_HOST and DB_PASSWORD for your airflow backend and any other secrets you wish to provide
    # to your environment.
    createFile: true
    # airflow.secrets.secretName -- a secrets file providing DB_HOST and DB_PASSWORD
    # for your airflow backend and any other secrets you wish to provide  to your environment.
    secretName: "secrets-default"
    # airflow.secrets.extraSecrets -- The name of a secret object in the same namespace. This will contain
    # DB_PASSWORD, DB_HOST, REMOTE_LOGGING_STRING
    #    secret_file: my-secrets
    #    secret_file:
    # Any Further Secrets that need to be available in the environment can be added here.
    # IF PROVIDING FILE:
    # Secret only needs to a list form e.g
    #    extra_secrets:
    #      - Secret1
    #      - Secret2
    # IF SECRETS TO BE CREATED BY THIS HELM CHART (not recommended for production):
    # Secrets to be provided as a map e.g
    #      extra_secrets:
    #        Secret1: "admin"
    #        Secret2: "otherpass"
    extraSecrets: []
  worker:
    # airflow.worker.pods_per_heartbeat -- number of pods to create per scheduler heartbeat
    podsPerHeartbeat: 5
    # airflow.worker.envVars -- Additional Environment variables that will be created on workers when they initialise
    envVars: {}
  # airflow.envVars -- Extra Variables to create in your webserver and scheduler containers.
  envVars: {}
  db:
    # airflow.db.create -- Create a postgres backend for your airflow cluster. More suitable for
    # testing than for production.
    create: true
    protocol: postgresql+psycopg2
    username: admin
    host: airflow-default-db-svc
    password: testadminpass
    dbname: airflow
    port: 5432

